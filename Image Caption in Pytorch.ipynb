{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab11303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90202edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "232353b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec81c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_english = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0b853c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a good place to find a city\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72d13efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'good', 'place', 'to', 'find', 'a', 'city']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text.lower() for token in spacy_english.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6261d",
   "metadata": {},
   "source": [
    "### Defining a Vocabulary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "286d2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "151e2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self,freq_count,tokenizer):\n",
    "        \n",
    "        self.word2idx = {\"<pad>\":0,\"<start>\":1,\"<end>\":2,\"<unk>\":3}\n",
    "        self.idx2word = {v:k  for k,v in self.word2idx.items()}\n",
    "        self.tokenizer = tokenizer\n",
    "     \n",
    "        self.freq_threshold = freq_count\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "    \n",
    "    def tokenize_text(self,text):\n",
    "        \n",
    "        return [token.text.lower() for token in self.tokenizer(text)]\n",
    "    \n",
    "    def build_dictionary(self,sentences:list):\n",
    "        \"\"\"Takes in sentences of list as input\n",
    "        \"\"\"\n",
    "        index = 4\n",
    "        word_count = Counter()\n",
    "        \n",
    "        for sent in sentences:\n",
    "            for word in self.tokenize_text(sent):\n",
    "                word_count[word]+=1\n",
    "                \n",
    "                if word_count[word] == self.freq_threshold:\n",
    "                    self.word2idx[word]=index\n",
    "                    self.idx2word[index]=word\n",
    "                    index+=1\n",
    "                    \n",
    "    def encode_text(self,text):\n",
    "        \n",
    "        tok_text = self.tokenize_text(text)\n",
    "        \n",
    "        return [self.word2idx[word] if word in self.word2idx.keys() else self.word2idx['<unk>']  for word in tok_text]\n",
    "    \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd21835d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "004709d0",
   "metadata": {},
   "source": [
    "### Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4c8965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c17a21a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('captions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdb3e16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9a7a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab =Vocab(1,spacy_english.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45c30613",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.build_dictionary(list(data['caption']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c0a92eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8508"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c12b427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a64e0b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 458, 3460, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.encode_text(\"Heyllo I am daniyal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f587b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ea7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16d61926",
   "metadata": {},
   "source": [
    "### Reading Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23971de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d161b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ebd2394",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = data['image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c990852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1000268201_693b08cb0e.jpg'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "077c3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(image_path+\"/\"+image).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4193b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca63b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57058049",
   "metadata": {},
   "source": [
    "### Spliting Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af30cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = \\\n",
    "              np.split(data.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(data)), int(.8*len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "572f6199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24273, 8091, 8091, 40455)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train),len(validate),len(test),len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4a6c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13380aae",
   "metadata": {},
   "source": [
    "### Creating Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2c20120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f83d2cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sent,max_len,pad_token):\n",
    "    \n",
    "    pad_num = max_len - len(sent)\n",
    "    sent.extend([pad_token for i in range(pad_num)])\n",
    "    \n",
    "    return sent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba852b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self,image_path,df,transform,vocab,max_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_path = image_path\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        data_item = self.df.iloc[idx]\n",
    "        \n",
    "        image = Image.open(self.image_path + \"/\" + data_item['image']).convert(\"RGB\")\n",
    "        \n",
    "        image = self.transform(image)\n",
    "        \n",
    "        text = [self.vocab.word2idx['<start>']]\n",
    "        text.extend(self.vocab.encode_text(data_item['caption']))\n",
    "        train_caption = text.copy()\n",
    "        text.append(self.vocab.word2idx['<end>'])\n",
    "        \n",
    "        text = pad_sequence(text,self.max_len,self.vocab.word2idx['<pad>'])\n",
    "        train_caption = pad_sequence(train_caption,self.max_len,self.vocab.word2idx['<pad>'])\n",
    "        \n",
    "        return image,torch.tensor(text,dtype=torch.long),\\\n",
    "                torch.tensor(train_caption,dtype=torch.long)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb330e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "  T.Resize(226),                     \n",
    "    T.RandomCrop(224),                 \n",
    "    T.ToTensor(),                               \n",
    "    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd48a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset= CaptionDataset(image_path,\n",
    "                              train,\n",
    "                              transform,\n",
    "                              vocab,\n",
    "                              42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa6275f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset= CaptionDataset(image_path,\n",
    "                              validate,\n",
    "                              transform,\n",
    "                              vocab,\n",
    "                              42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e43d997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset= CaptionDataset(image_path,\n",
    "                              test,\n",
    "                              transform,\n",
    "                              vocab,\n",
    "                              42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea401371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77ed8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image,text,train_caption = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2b06713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), torch.Size([42]), torch.Size([42]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size(),text.size(),train_caption.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaaebcf",
   "metadata": {},
   "source": [
    "### Creating DataLoader for Pytorch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aafe5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0557e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,batch_size=BATCH_SIZE)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6e805e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eb85ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a99bb6be",
   "metadata": {},
   "source": [
    "## Creating a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1785ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d2a20ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniyal\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Daniyal\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model_res = torchvision.models.resnet50(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a38c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model_res.children())[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d38660e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "759b6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,embedding_size):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        pretrained = torchvision.models.resnet50(pretrained = True)\n",
    "        for param in pretrained.parameters():\n",
    "            param.requires_grad_(False)\n",
    "            \n",
    "        self.resnet = nn.Sequential(*(list(pretrained.children())[:-1]))\n",
    "        self.features = nn.Linear(pretrained.fc.in_features,embedding_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(embedding_size)\n",
    "        \n",
    "        \n",
    "    def forward(self,image):\n",
    "        \n",
    "        feat = self.resnet(image)\n",
    "        feat = feat.view(feat.size(0),-1)\n",
    "        feat = self.features(feat)\n",
    "        feat = self.batch_norm(feat)\n",
    "        \n",
    "        return feat\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ff70e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da59313a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 200])\n"
     ]
    }
   ],
   "source": [
    "sample = torch.rand((2,3,224,224))\n",
    "out = enc(sample)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13ddebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,embed,hidden,vocab_size,layers= 1):\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.embedding_size = embed\n",
    "        self.hidden_size = hidden\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size,self.embedding_size,\\\n",
    "                                      padding_idx =0)\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.embedding_size,self.hidden_size,self.layers)\n",
    "        self.final_layer = nn.Linear(self.hidden_size,self.vocab_size)\n",
    "        \n",
    "    def forward(self,feats,train_captions):\n",
    "#         print(feats.size())\n",
    "        feats = feats.unsqueeze(1)\n",
    "#         print(feats.size())\n",
    "        embeddings = self.embedding(train_captions)\n",
    "        embeddings = torch.cat((feats,embeddings[:,:-1,:]),dim=1)\n",
    "        \n",
    "        h,_ = self.lstm(embeddings)\n",
    "        final_output = self.final_layer(h)\n",
    "        \n",
    "        return final_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d156712b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab344bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,vocab,embedding_size,hidden_dim,num_layers=1):\n",
    "        super(EncoderDecoder,self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(embedding_size)\n",
    "        self.decoder = Decoder(embedding_size,hidden_dim,vocab,num_layers)\n",
    "        \n",
    "    def forward(self,image,caption):\n",
    "        \n",
    "        feat = self.encoder(image)\n",
    "        out = self.decoder(feat,caption)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def predict_caption(self,image,vocab,max_length =40):\n",
    "        \n",
    "        caption = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.encoder.eval()\n",
    "            var = self.encoder(image.unsqueeze(0)).unsqueeze(0)\n",
    "            h_c = None\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                hidden_out,h_c = self.decoder.lstm(var,h_c)\n",
    "#                 print(hidden_out.size())\n",
    "                output = self.decoder.final_layer(hidden_out.squeeze(0))\n",
    "#                 print(output.size())\n",
    "                pred = output.argmax(1)\n",
    "                caption.append(pred.detach().cpu().item())\n",
    "                var =self.decoder.embedding(pred).unsqueeze(0)\n",
    "                \n",
    "                if vocab.idx2word[pred.item()] == \"<end>\":\n",
    "                    break\n",
    "            return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e84a2b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = EncoderDecoder(vocab = 8508,embedding_size = 128,\n",
    "                        hidden_dim = 128,num_layers =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e8aa122",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image,sample_caption = torch.rand((5,3,224,224)),torch.randint(0,8507,(5,42),dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c1740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c877516",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = testing(sample_image,sample_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb91d614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f09fa23",
   "metadata": {},
   "source": [
    "### Zero Shot Caption Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2c2fb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image,_,_ = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b3c313d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4237ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = testing.predict_caption(image,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5bf3b0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pedestrian', 'stringless', 'multistory', 'dandilions', 'hat', 'min', 'great', 'skips', 'confetti', 'intently', 'racquet', 'kneels', 'elder', 'lots', 'rollercoaster', 'elder', 'purchasing', 'board', 'emerging', 'promting', 'shirtness', 'iceskate', 'stood', 'wipes', 'soil', 'nipples', 'skiiers', 'skies', 'divided', 'casque', 'casque', 'teeth', 'sheets', 'waterskis', 'oar', 'midway', 'programs', 'rappeling', 'marketplace', 'waterspouts']\n"
     ]
    }
   ],
   "source": [
    "caption = [vocab.idx2word[c] for c in caption]\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3facb85d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4231bed2",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd25257",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "839104ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-4    \n",
    "embedding_size = 256\n",
    "hidden_dim = 256\n",
    "num_layers = 1\n",
    "epochs = 100\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = vocab.word2idx['pad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a7f93",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b0b0b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoder(vocab = 8508,embedding_size = embedding_size,\n",
    "                        hidden_dim = hidden_dim,num_layers =num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcfe247",
   "metadata": {},
   "source": [
    "#### Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e6a902d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3b330b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9174b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bcc85307",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(),lr= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cfc9985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_fn(model,epochs,dataset,loss_fn,optim,schedular = None):\n",
    "    \"\"\"\n",
    "    model: The CNN and LSTM(Encoder Decoder Function)\n",
    "    epochs: Number of iteration of training\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_values = []\n",
    "    \n",
    "    for i in tqdm(range(epochs)):\n",
    "        loss_per_batch = []\n",
    "        for (image,output_text,train_text) in dataset:\n",
    "            image,output_text,train_text = image.to(device),output_text.to(device),train_text.to(device)\n",
    "            \n",
    "            output = model(image,train_text)\n",
    "            \n",
    "            loss = loss_fn(output.reshape(-1,output.shape[2]),output_text.reshape(-1))\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            loss_per_batch.append(loss.detach().cpu().item())\n",
    "        print('Epoch {}: Train loss: {}'.format(i,np.mean(loss_per_batch)))\n",
    "        loss_values.append(np.mean(loss_per_batch))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7f5ca48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.0569, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.0575, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.0502, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.0455, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.0381, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.0332, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.0328, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.0271, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.0112, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss_values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_train_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [71]\u001b[0m, in \u001b[0;36mmodel_train_fn\u001b[1;34m(model, epochs, dataset, loss_fn, optim, schedular)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (image,output_text,train_text) \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m     14\u001b[0m     image,output_text,train_text \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device),output_text\u001b[38;5;241m.\u001b[39mto(device),train_text\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 16\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]),output_text\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36mEncoderDecoder.forward\u001b[1;34m(self, image, caption)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,image,caption):\n\u001b[1;32m---> 10\u001b[0m     feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(feat,caption)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,image):\n\u001b[1;32m---> 16\u001b[0m     feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     feat \u001b[38;5;241m=\u001b[39m feat\u001b[38;5;241m.\u001b[39mview(feat\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m     feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(feat)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\resnet.py:155\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m    154\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(out)\n\u001b[1;32m--> 155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_values = model_train_fn(model,epochs,train_dataloader,loss_fn,optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d046e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d43a271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e1035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2901d21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85279d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b4968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee21f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cae76a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
